{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b84509f-47fc-4289-abb4-552c24db0edb",
   "metadata": {},
   "source": [
    "# Install necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdee93-a8f1-4f9e-bbf3-d0df63eabeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib SPARQLWrapper bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f520c-9779-4daa-9606-50dd255afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd70e4e-8ec5-4ed3-bc60-6aac35a89707",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0df5a6-a13a-430f-96b4-4f57b361f12e",
   "metadata": {},
   "source": [
    "# Install an LLM.\n",
    "on your own machine\n",
    "go to https://github.com/ollama/ollama and download the framework for your platform. This is the free(but not Open) Model From Meta\n",
    "\n",
    "https://en.wikipedia.org/wiki/Llama_(language_model)\n",
    "\n",
    "There are many other ways to get local LLMs and RAGs running. \n",
    "- [Huggingface](https://huggingface.co/models) is the headquarter for all sorts of models\n",
    "- [MLStudio](https://lmstudio.ai/) and [GPT4All](https://gpt4all.io/index.html) are two of the prominent GUI interfaces to make installation and use a breeze.\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae4eee-6e32-486e-a421-62cd025a7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"mistral\"\n",
    ")\n",
    "print(ollama.invoke(\"Who is in the programm committee of the LDAC 2024 workshop?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0e8d2-f39a-4ef4-be75-6f6bfeb625e8",
   "metadata": {},
   "source": [
    "# Ingest\n",
    "Lets retrieve some information to use as embeddings\n",
    "Why don't we start with our own context: LDAC 2024! Yay!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678001e3-f831-4a7a-b202-e83fb055a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://linkedbuildingdata.net/ldac2024/\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc7046-b52c-4ed3-8fd5-fb5a32c6a5fd",
   "metadata": {},
   "source": [
    "In order to be fed to our RAG, let's chop up the html into tokens. There are a lot of tokenizers to adress different types of input data, e.g. PDFs etc. \n",
    "In this case we are dealing with HTML and we e.g. don't want to polute our embeddings with html tags like `<b> <pre> <div>` , javascript etc. \n",
    "\n",
    "Langchain has us covered with a number of different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34584fc-16f9-4523-98fd-c8b73a7ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca3968-aad5-4a64-9094-de47528d1042",
   "metadata": {},
   "source": [
    "The raw daata is ready, we can feed it into a vector store. We are using Chroma. There are a lot of different options available, and two months from the time of this writing, things will have broken already... we are living in very dynamic times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326103a6-83ba-4a82-a807-7981dab1d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0972378-0459-4e58-9478-bde35342b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"mistral\"\n",
    ")\n",
    "print(ollama.invoke(\"what is the LDAC and who is part of the committee\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017e80f-7c63-40cf-8e53-71f63fd37e66",
   "metadata": {},
   "source": [
    "Let's see what documents the vectorstore finds for similarities for our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850168a-24d9-4c24-834a-66e67c5e836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"who is in the programm committee of the LDAC 2024?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bc115-bb29-4d81-b636-27964e916d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75286988-df48-4001-a32a-a719f9d48258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever())\n",
    "res = qachain.invoke({\"query\": question})\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4447018-d5fd-45fe-9d61-2f00e69fd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"what is the semantic scope with respect to data of the LDAC\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever())\n",
    "res = qachain.invoke({\"query\": question})\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a31fc9-1969-4b80-bc64-865110627026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def save_vectorstore_to_file(vectorstore, filename):\n",
    "    # Extract documents and metadata\n",
    "    documents = vectorstore._collection.get()[\"documents\"]\n",
    "    metadatas = vectorstore._collection.get()[\"metadatas\"]\n",
    "\n",
    "    # Prepare data for serialization\n",
    "    data_to_save = {\n",
    "        \"documents\": documents,\n",
    "        \"metadatas\": metadatas\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data_to_save, f)\n",
    "\n",
    "# Save the vectorstore to a JSON file\n",
    "save_vectorstore_to_file(vectorstore, 'vectorstore_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35328047-3042-4499-a7bd-3abc7cd1d064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
